Laravel 12 Gemini API Integration

Este proyecto es una implementaci√≥n de backend utilizando Laravel 12 que act√∫a como un wrapper/proxy seguro para interactuar con la Inteligencia Artificial de Google, espec√≠ficamente el modelo Gemini 2.5 Flash Lite.

El sistema est√° dise√±ado para ser Stateless y compatible con entornos Serverless como Vercel.

üåê Demo en Vivo

La API se encuentra desplegada y operativa en Vercel:

Base URL: https://chatgpt-api-ruby.vercel.app

Endpoint

M√©todo

Descripci√≥n

Estado

/api/gemini/chat

POST

Chat con la IA

‚úÖ Activo

/api/gemini/health

GET

Verificaci√≥n de servicio

‚úÖ Activo

üöÄ Caracter√≠sticas

Integraci√≥n con Gemini 2.5 Flash Lite: Utiliza la √∫ltima versi√≥n ligera y r√°pida del modelo.

Serverless Ready: Configurado para funcionar sin persistencia de archivos locales (Vercel/AWS Lambda).

Validaci√≥n de Datos: Reglas estrictas para message, temperature y maxTokens.

Manejo de Errores Robusto: Control de excepciones, tiempos de espera (timeouts) y reintentos autom√°ticos.

Seguridad: API Key protegida en el servidor; el cliente nunca la ve.

üõ†Ô∏è Requisitos Previos (Local)

PHP 8.2 o superior.

Composer.

Una API Key de Google AI Studio.

‚öôÔ∏è Instalaci√≥n Local

Clonar el repositorio

git clone <https://github.com/GAMR11/chatgpt-api.git>
cd <chatgpt-api>


Instalar dependencias

composer install


Configurar el entorno

cp .env.example .env
php artisan key:generate


Configurar la API Key
En tu archivo .env:

GEMINI_API_KEY="tu_api_key_aqui"
SESSION_DRIVER=cookie  # Importante para simular entorno serverless


üîå Documentaci√≥n de la API

1. Chat con Gemini

Env√≠a un mensaje al modelo y recibe una respuesta generada.

URL Producci√≥n: https://chatgpt-api-ruby.vercel.app/api/gemini/chat

URL Local: http://localhost:8000/api/gemini/chat

M√©todo: POST

Cuerpo de la Solicitud (JSON):

Par√°metro

Tipo

Requerido

Descripci√≥n

Restricciones

message

string

S√≠

El prompt para la IA.

M√°x 5000 caracteres.

temperature

float

No

Creatividad.

0.0 a 2.0 (Default: 0.7).

maxTokens

integer

No

Longitud m√°x.

1 a 8192 (Default: 2048).

Ejemplo de uso (cURL):

curl -X POST [https://chatgpt-api-ruby.vercel.app/api/gemini/chat](https://chatgpt-api-ruby.vercel.app/api/gemini/chat) \
-H "Content-Type: application/json" \
-H "Accept: application/json" \
-d '{
    "message": "Escribe un poema corto sobre programaci√≥n",
    "temperature": 1.0,
    "maxTokens": 500
}'


Respuesta Exitosa (200 OK):

{
    "success": true,
    "data": {
        "message": "C√≥digo en pantalla,\nluz en la oscuridad,\nun bug se escapa,\n¬°caf√© y libertad!",
        "model": "gemini-2.5-flash-lite"
    }
}


2. Health Check

URL: /api/gemini/health

M√©todo: GET

{
    "status": "ok",
    "service": "gemini",
    "model": "gemini-2.5-flash-lite",
    "api_key_configured": true
}


‚òÅÔ∏è Detalles del Despliegue en Vercel

Este proyecto tiene configuraciones espec√≠ficas para correr en una arquitectura Serverless:

Almacenamiento Ef√≠mero: No se usa SQLite ni almacenamiento local de archivos.

Sesiones & Cach√©:

SESSION_DRIVER: Configurado como cookie (las sesiones viajan encriptadas al navegador).

CACHE_DRIVER: Configurado como array (la cach√© vive solo lo que dura la petici√≥n).

Configuraci√≥n Vercel:

Se utiliza un archivo vercel.json para redirigir el tr√°fico al index.php de Laravel.

Los logs se redirigen a stderr para ser visibles en el dashboard de Vercel.

Estructura de Archivos Clave

api/index.php: Punto de entrada para el runtime de Vercel.

vercel.json: Configuraci√≥n de rutas y entorno.

üõ°Ô∏è Seguridad

Bloqueo de Contenido: Filtros de seguridad activos (HARM_CATEGORY_HARASSMENT, etc.).

Logs: Errores registrados sin exponer datos sensibles.

üìÑ Licencia

Este proyecto es de c√≥digo abierto bajo la licencia MIT.